{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg;\n",
    "using Statistics;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6363636363636364, 0.6388888888888888, 0.7000000000000001, 0.6680497925311203, [3.0 1.0 1.0; 1.0 2.0 1.0; 0.0 0.0 2.0])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets = convert(AbstractArray{Bool,2},[1 1 0 0 1 1 0 1 0 0 0; 0 0 1 0 0 0 1 0 1 0 1;0 0 0 1 0 0 0 0 0 1 0])\n",
    "outputs = convert(AbstractArray{Bool,2},[1 0 0 0 0 1 0 1 1 0 0; 0 1 1 0 0 0 0 0 0 0 1;0 0 0 1 1 0 1 0 0 1 0])\n",
    "\n",
    "(accuracy,precision, recall,fScore, mcm) = multiclassConfusionMatrix(outputs,targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "multiclassConfusionMatrix (generic function with 1 method)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function multiclassConfusionMatrix(outputs::AbstractArray{Bool,2}, targets::AbstractArray{Bool,2}; weighted::Bool=true)\n",
    "    \n",
    "    targets = targets'\n",
    "    outputs = outputs'\n",
    "\n",
    "    num_columns = size(targets,2)\n",
    "    confusion_matrix= zeros(num_columns,num_columns)\n",
    "  \n",
    "      #confusion matrix\n",
    "    for x in 1:num_columns\n",
    "        for y in 1:num_columns\n",
    "            for c in 1:(size(outputs,1))\n",
    "                if outputs[c,x] == true\n",
    "                    if outputs[c,x] == targets[c,y]\n",
    "                        confusion_matrix[y,x] =confusion_matrix[y,x] + 1\n",
    "                    end\n",
    "                end\n",
    "            end  \n",
    "        end\n",
    "    end\n",
    "\n",
    "    mcm = confusion_matrix\n",
    "\n",
    "    TP = zeros(size(mcm,1))\n",
    "    precision = zeros(size(mcm,1))\n",
    "    recall = zeros(size(mcm,1))\n",
    "\n",
    "    for index in 1:size(mcm,1)\n",
    "        TP[index] = mcm[index,index]\n",
    "        STrow = sum(mcm[index,:])\n",
    "        STcolumn = sum(mcm[:,index])\n",
    "        precision[index] = TP[index]/STcolumn\n",
    "        recall[index] = TP[index]/STrow\n",
    "    end\n",
    "\n",
    "    model_accuracy = sum(TP)/sum(mcm)\n",
    "    model_precision = mean(precision)\n",
    "    model_recall = mean(recall)\n",
    "    model_fScore = (2*model_precision*model_recall)/(model_precision+model_recall)\n",
    "\n",
    "    return (model_accuracy,model_precision, model_recall,model_fScore, mcm)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function multiclassConfusionMatrix(outputs::AbstractArray{Bool,2}, targets::AbstractArray{Bool,2};metric::Symbol=:all)\n",
    "    print(\"hola\")\n",
    "    (model_accuracy,model_precision, model_recall,model_fScore, mcm) = multiclassConfusionMatrix(convert(AbstractArray{Bool,2},outputs),convert(AbstractArray{Bool,2},targets))\n",
    "\n",
    "    if (metric == :all)\n",
    "        return (model_accuracy,model_precision, model_recall,model_fScore, mcm)\n",
    "    elseif (metric == :af)\n",
    "        return (model_accuracy,model_fScore)\n",
    "    elseif (metric == :pr)\n",
    "        return (model_accuracy,model_fScore)\n",
    "    elseif (metric == :afm)\n",
    "        return (model_accuracy,model_fScore)\n",
    "    elseif (metric == :m)\n",
    "        return (model_accuracy,model_fScore)\n",
    "    else\n",
    "        return (model_accuracy,model_precision, model_recall,model_fScore, mcm)\n",
    "    end\n",
    "end\n",
    "\n",
    "\n",
    "function confusionMatrix(outputs::AbstractArray{Bool,1}, targets::AbstractArray{Bool,1})\n",
    "\n",
    "    @assert(length(outputs)==length(targets))\n",
    "\n",
    "    TN = sum(.!outputs .& .!targets);\n",
    "    FN = sum(.!outputs .& targets);\n",
    "    TP = sum(outputs .&  targets)\n",
    "    FP = sum(outputs .& .!targets)\n",
    "    \n",
    "    confMatrix = [TN FP; FN TP]\n",
    "    \n",
    "    #Metrics\n",
    "    \n",
    "    if ((TN+FN+TP+FP)==0)\n",
    "        accuracy = 0\n",
    "        error_rate = 0\n",
    "    else\n",
    "        accuracy = (TN+TP)/(TN+FN+TP+FP)\n",
    "        error_rate = (FN+FP)/(TN+FN+TP+FP)\n",
    "    end\n",
    "\n",
    "    if (TN==length(outputs))\n",
    "        recall = 1 #Sensitivity\n",
    "        precision = 1 #Positive predicitive value\n",
    "        specificity = TN/(FP+TN)\n",
    "        negative_predicitve_value = TN/(TN+FN)\n",
    "    elseif(TP==length(outputs))\n",
    "        recall = TN/(FN+TP)\n",
    "        precision = TP/(TP+FP)\n",
    "        specificity = 1\n",
    "        negative_predicitve_value = 1\n",
    "    elseif(FN+TP == 0)\n",
    "        recall = 0\n",
    "        precision = TP/(TP+FP)\n",
    "        specificity = TN/(FP+TN)\n",
    "        negative_predicitve_value = TN/(TN+FN)\n",
    "    elseif(TP+FP == 0)\n",
    "        recall = TN/(FN+TP)\n",
    "        precision = 0\n",
    "        specificity = TN/(FP+TN)\n",
    "        negative_predicitve_value = TN/(TN+FN)\n",
    "    elseif(FP+TN == 0)\n",
    "        recall = TN/(FN+TP)\n",
    "        precision = TP/(TP+FP)\n",
    "        specificity = 0\n",
    "        negative_predicitve_value = TN/(TN+FN)\n",
    "    elseif(TN+FN == 0)\n",
    "        recall = TN/(FN+TP)\n",
    "        precision = TP/(TP+FP)\n",
    "        specificity = TN/(FP+TN)\n",
    "        negative_predicitve_value = 0\n",
    "    else\n",
    "        recall = TN/(FN+TP)\n",
    "        precision = TP/(TP+FP)\n",
    "        specificity = TN/(FP+TN)\n",
    "        negative_predicitve_value = TN/(TN+FN)\n",
    "    end\n",
    "    (precision==0 && recall==0) ? fscore = 0 : fscore = (2*recall*precision)/(recall+precision)\n",
    "      \n",
    "    return (confMatrix,accuracy,error_rate,recall,specificity,precision,negative_predicitve_value,fscore);\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.2",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
